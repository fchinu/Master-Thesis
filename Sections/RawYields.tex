\chapter{\texorpdfstring{\ds and \dpl signal extraction}{Ds+ and D+ signal extraction}}

The main ingredient for the evaluation of the \ds/\dpl production-yield ratio is the \ds and \dpl raw yield, i.e., the number of reconstructed \ds and \dpl mesons. Due to the vast amount of combinatorial background and the limited efficiency of about 1\%, extracting the raw yield through a candidate counting method is not feasible. Instead, the raw yield is obtained on a statistical basis by fitting the invariant-mass distribution of the \ds and \dpl candidates passing tight selection criteria. To reduce the combinatorial background and enhance the efficiency of D-meson selection, Machine Learning algorithms have been employed. The following sections describe the procedure for the extraction of the raw yield of \ds and \dpl mesons. 


\section{Machine Learning}
The term \emph{Machine Learning} (ML) is a broad and versatile concept, encompassing a wide range of algorithms that grant computers the capacity to learn and adapt without being explicitly programmed to do so~\cite{5392560}. A more comprehensive definition characterises ML as the study of algorithms that enhance their performance at a specific task through the accumulation of experience~\cite{mitchell1997machine}. In recent years, ML techniques have witnessed widespread adoption across diverse fields, with significant impacts realised especially with the emergence of generative models such as GPT~\cite{openai2023gpt4}. ML algorithms have found extensive applications in the high-energy physics field, primarily for the task of distinguishing interesting signals from the vast background present in particle-collision data. Furthermore, these algorithms have been employed as triggers, aiding in the rapid identification of events of interest, and have also been instrumental in event reconstruction. Notably, ML algorithms were used in the discovery of the Higgs boson~\cite{CMS:2012qbp}, one of the most significant achievements in the field of particle physics of the last decades.



\subsection{Supervised learning}
Supervised learning is one of the main branches of machine learning, along with unsupervised and reinforcement learning. Machine learning tasks are usually described in terms of how the machine learning system should process an example, which is a collection of features $\mathbf{x}$ that have been quantitatively measured from some object or event that one wants the machine learning system to process. In the case of supervised learning, each example is coupled with a corresponding label or target, $\mathbf{y}$. The objective of supervised learning is to learn to predict or infer $\mathbf{y}$ based on the associated features, $\mathbf{x}$, assuming that there exists a functional relationship $\mathbf{y} = f(\mathbf{x})$ between the two. The goal of the ML system is to produce an approximation $\widehat{f}(\mathbf{x})$ of the true function $f(\mathbf{x})$ by minimising a given loss function, which quantifies the discrepancy between the predicted and true labels. Supervised learning problems are further segmented into two distinct sub-categories: classification and regression. In the former, the label $\mathbf{y}$ assumes values from a finite and discrete set of categories, often representing distinct classes or groups. In the latter, the label $\mathbf{y}$ takes the form of one or more continuous variables, necessitating the learning system to deduce a continuous function or mapping between $\mathbf{x}$ and $\mathbf{y}$.

The usage of ML algorithms in classification problems, such as the one presented in this Thesis, allows for the definition of multi-dimensional non-linear decision boundaries, which are not available with traditional selection methods. This is particularly important as it provides more efficient selections and a larger purity of the selected data sample.

The application of a supervised learning algorithm to a dataset involves the following steps: i. the model is trained on a set of labelled data, i.e., the value of $\mathbf{y}$ is known for each example in the training set; ii. the model is tested on a separate set of labelled data, known as the test set, to evaluate its performance; iii. the model is then used to make predictions on new, unseen data.

\subsubsection{Training}
During the training process, the model learns (i.e., adjusts its internal parameters) to map the input features $\mathbf{x}$ to the corresponding labels $\mathbf{y}$ by minimizing a given loss function. Typically used loss functions include the Mean Squared Error (MSE) for regression tasks and the Cross-Entropy loss~\cite{mao2023cross} for classification tasks. The loss function is minimised through an optimisation algorithm, usually stochastic gradient descent~\cite{10.1214/aoms/1177729392}, which iteratively updates the model parameters to reduce the loss. Since an over-optimisation of the model on the training data can lead to poor generalisation on unseen data (the model is said to be \emph{overfitting}), a regularisation term is often added to the loss function to penalise overly complex models. The training process continues until the model reaches a satisfactory level of performance on the training data, or until its performance does not improve further. 

Before the final model is trained, hyperparameters tuning is performed to optimise the model's performance. \emph{Hyperparameters} are parameters that are not learned during the training process, but rather define the model's architecture and the training process itself. Hyperparameters tuning is usually performed through a grid search, random search, or with a more efficient bayesian optimisation~\cite{frazier2018tutorial,snoek2012practical,mockus2005bayesian}. Several combinations of hyperparameters are tested on a dedicated labelled dataset: the validation set. Models with different hyperparameter sets are trained with a reduced training phase, and those yielding the best performance are then selected for the final model training.

\subsubsection{Testing}
After the model has been trained, its performance is evaluated on a dataset that was not used during the training process, known as the test set. Like the training and validation sets, also the test set contains labelled examples. While during the training the model is optimised to minimise the loss function, the test set is used to estimate the model's generalisation error, i.e., how well the model performs on unseen data. The model's performance is evaluated using metrics that are specific to the task at hand, such as accuracy for classification tasks, or Mean Squared Error (MSE) for regression tasks. Once the model achieves satisfactory performance on the test set, it is ready to be used for making predictions on unlabelled data.

\subsubsection{Cross-validation}

With the strategy defined above to optimise the hyperparameters, train the model and validate its performance, the dataset is divided into three subsets: the training set, the validation set, and the test set. When small datasets are involved, this division can lead to a suboptimal model, as the model's performance can be highly dependent on the specific examples in the training, validation, and test sets. Furthermore, this approach limits the amount of data available for training the model, which can lead to poor generalisation. To mitigate this issue, cross-validation~\cite{stone1974cross} is often employed. This term refers to a set of techniques that allow for a more robust estimate of the model's generalisation performance by using the entire dataset for training and validation. The most common cross-validation technique is the $k$-fold cross-validation. It consists in dividing the training sample into $k$ subsets of equal size, called \emph{folds}. Then, the ML algorithm is trained $k$ times, each time using $k-1$ folds as training set, while the remaining fold is used as validation set. The model's performance is then averaged over the $k$ folds to obtain a more robust estimate of this quantity. This operation is repeated for each hyperparameter configuration to be considered. The hyperparameter configuration minimising the loss function is then chosen as the optimal configuration.

\section{\texorpdfstring{\ds and \dpl selection using Machine Learning}{Ds+ and D+ selection using Machine Learning}}

The task of extracting \ds and \dpl signals from the vast combinatorial background is a challenging one, due to the large amount of background compared to signal. It is however an excellent example of classification problem, and ML algorithms can therefore be exploited to enhance the efficiency of the selection. 

\subsection{Data preparation}\label{sec:ml_data_preparation}
In order to train a ML model, a labelled dataset with a well-defined set of features is required. The dataset used for training the ML algorithms employed in this Thesis is composed of a number of signal and background examples. To obtain a pure sample of signal candidates, Monte Carlo techniques are used to generate \ds and \dpl mesons. Proton-proton collisions are simulated using the \textsc{Pythia~8} event generator~\cite{Bierlich:2022pfr} with colour-reconnection Mode~2~\cite{Christiansen:2015yqa}, and the generated particles are propagated through the ALICE detector using the \textsc{Geant4} transport simulation toolkit~\cite{GEANT4:2002zbu}. To enrich the sample of heavy-flavour hadrons, $\mathrm{c\overline{c}}$ and $\mathrm{b\overline{b}}$ pairs are injected into each simulated event. 


%Due to the displaced topology of heavy-flavour decays and the continuous readout employed by the ALICE detector, the selection of events with charm or beauty hadrons produces ``fake'' vertices arising from the association of displaced decay tracks, affecting the reconstruction of heavy-flavour hadrons. To overcome this problem, minimum bias events are generated between charm or beauty enriched ones (\emph{gap-triggered} approach). Studies performed using different gap sizes have shown that a gap of 5 minimum bias events reduces the fake-vertex rate to an acceptable level, while keeping the simulation time reasonable. 

Only prompt and non-prompt \ds mesons are used to train the model, as \dpl mesons decay into the same final state as \ds mesons, and selections optimised to reconstruct \ds mesons are also effective for \dpl mesons. 

Background candidates are obtained from real data, as MC simulations may not be able to reproduce the complexity of soft processes occurring in the underlying event, or may not be able to model the detector response accurately. Background examples are obtained by selecting candidates from a subsample of the full data sample (corresponding to its 3\%) in an invariant-mass region away from both the \ds and \dpl mass peaks, where $1.7 < M < 1.75$~\gevcc or \mbox{$2.1 < M < 2.15$~\gevcc}, as shown in Fig.~\ref{fig:ml_training_mass}.
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter 5/Mass.pdf}
    \caption{Invariant mass distribution of prompt and non-prompt D-mesons (blue and orange, respectively), taken from Monte Carlo simulations, and of the background candidates taken from real data used to train the ML model (green) in the $4<\pt<6$~\gevc interval. Background candidates are selected in the $1.7 < M < 1.75$~\gevcc or \mbox{$2.1 < M < 2.15$~\gevcc} invariant-mass interval.}
    \label{fig:ml_training_mass}
\end{figure}

To further reduce the amount of combinatorial background in the data samples, some loose selections were applied. These pre-selections are very effective at rejecting the combinatorial background, while preserving the selection efficiency for \ds and \dpl mesons, and are reported in Table~\ref{tab:presel}. A selection is applied to the $\chi^2_\mathrm{PCA}$ variable, quantifying the dispersion of the decay tracks around the secondary vertex, normalised to their uncertainty.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{c|ccc}
        \toprule
        & \multicolumn{3}{c}{\pt interval~(\gevc)}\\
        \midrule
       Variable & 0.0$-$1.5 & 1.5$-$12 &  12$-$24\\
        \midrule
      $\lvert M - M_\ds^\mathrm{PDG} \rvert~(\mevcc) <$ & 400 & 400 & 400\\
      $\pt(\pi, \mathrm{K})~(\gevc) >$  & 0.3   & 0.4   & 0.4\\
      Decay length (cm) $>$             & 0.02  & 0.02  & 0.03\\
      Normalized decay length XY $>$      &2     & 2     & 2\\
      $\cos\theta_\mathrm{p}>$        & 0.85  & 0.85  & 0.85\\
      $\cos\theta_\mathrm{p}^{xy}>$            & 0.85  & 0.85  & 0.85\\
      $M^{\phi}_\mathrm{inv}$ - $M^{\phi~\mathrm{PDG}}_\mathrm{inv}~(\mevcc) <$ & 20 & 20 & 20\\
      $\chi^2_\mathrm{PCA} < $          & 10    & 10    & 10\\
      $\lvert n\sigma_\mathrm{TPC}\rvert <$                   & 5     & 5     & 5\\
      $\lvert n\sigma_\mathrm{TOF}\rvert <$                   & 5     & 5     & 5\\
      \bottomrule
    \end{tabular}
    \caption{Topological and kinematic pre-selections applied to the \ds- and \dpl-mesons candidates}
    \label{tab:presel}
  \end{center}
\end{table}

Labels are assigned as a numerical value to each candidate, with 0 indicating a background candidate, 1 a prompt \ds meson, and 2 a non-prompt \ds meson.

The dataset is then divided into two different subsamples. The first comprehends 80\% of the data, and is used to train the model, while the remaining 20\% is used to test its performance. In addition, since the D-meson decay topology can significantly differ depending on the \pt of the meson due to different Lorentz boosts, the dataset is divided into several \pt intervals, and the model is trained and tested separately for each of them. To achieve a better performance of the ML models, they are trained in broader \pt intervals than those used for the analysis, to ensure enough data is available to train a well-performing model. The total number of candidates available for training and testing the model is reported in Table~\ref{tab:training_sample} for the considered \pt intervals.


\begin{table}[htb]
    \begin{center}
    \caption{Number of candidates within the \pt intervals used to train and test the model.}
    \label{tab:training_sample}
    \vspace*{0.3cm}
    \begin{tabular}{c|ccc}
         \toprule
         \pt (\gevc) & Prompt \ds & Non-prompt \ds & Background\\
         \midrule         
         0--1.5     & $\sim 4.6 \times 10^{3}$  & $\sim 21  \times 10^{3}$   & $\sim 726  \times 10^{3}$ \\
         1.5--2     & $\sim 6.1 \times 10^{3}$  & $\sim 24  \times 10^{3}$   & $\sim 92   \times 10^{3}$\\
         2--3       & $\sim 26  \times 10^{3}$  & $\sim 96  \times 10^{3}$   & $\sim 123  \times 10^{3}$ \\
         3--4       & $\sim 34  \times 10^{3}$  & $\sim 124 \times 10^{3}$   & $\sim 114  \times 10^{3}$ \\
         4--5       & $\sim 31  \times 10^{3}$  & $\sim 113 \times 10^{3}$   & $\sim 63   \times 10^{3}$\\
         5--6       & $\sim 24  \times 10^{3}$  & $\sim 89  \times 10^{3}$   & $\sim 29   \times 10^{3}$\\
         6--8       & $\sim 32  \times 10^{3}$  & $\sim 115 \times 10^{3}$   & $\sim 22   \times 10^{3}$\\
         8--12      & $\sim 23  \times 10^{3}$  & $\sim 89  \times 10^{3}$   & $\sim 10   \times 10^{3}$\\
         12--24     & $\sim 9.7 \times 10^{3}$  & $\sim 39  \times 10^{3}$   & $\sim 2.6  \times 10^{3}$ \\
         \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\begin{sloppypar}
To produce a balanced dataset, the number of candidates in each class is equalised to the number of examples in the minority class. This is achieved by randomly selecting a subset of the majority classes. The balanced dataset is then used to train the model.
\end{sloppypar}

The choice of features used to separate signal from background is crucial, as they must be able to discriminate between signal and background candidates, and must be chosen in such a way that no bias is introduced in the final result. The variables used to train the model were introduced in Chapter~\ref{chap:reconstruction}, and are a mix of topological, kinematic, and PID variables. The key idea is to exploit the displaced topology of the D-meson decay, which is a distinctive feature of the signal candidates, the kinematic properties of the D-meson decay, and the PID information of the daughter tracks to discriminate between signal and background candidates. The features used to train the model are reported in Table~\ref{tab:ml_training_vars}. The number in parenthesis after $n\sigma$ indicates the prong number.

\begin{table}[htb]
    \begin{center}
    \caption{Candidate features used to train the ML model.}
    \label{tab:ml_training_vars}
    \vspace*{0.3cm}
    \begin{tabular}{c}
         \toprule
         Variable\\
         \midrule         
         cos$\theta_{p}$\\
         cos$\theta_{p}^{xy}$\\
         Decay length\\
         Decay length XY\\
         Candidate impact parameter XY\\
         $\lvert \cos^{3}\theta'(\mathrm K)\rvert$\\
         Prong 0 impact parameter XY\\
         Prong 1 impact parameter XY\\
         Prong 2 impact parameter XY\\
         $n\sigma_\mathrm{comb}^{\pi}(0)$\\         
         $n\sigma_\mathrm{comb}^{\pi}(1)$\\
         $n\sigma_\mathrm{comb}^{\pi}(2)$\\
         $n\sigma_\mathrm{comb}^{\mathrm K}(0)$\\
         $n\sigma_\mathrm{comb}^{\mathrm K}(1)$\\
         $n\sigma_\mathrm{comb}^{\mathrm K}(2)$\\
         \bottomrule
    \end{tabular}
    \end{center}
\end{table}

The invariant mass of the candidate and its \pt are not used to train the model. Exploiting such variables would introduce a bias in the final result, as the model would be trained to select candidates within a specific invariant mass region (that of \ds and \dpl mesons) or \pt. This would affect both the selection of the candidates and the \pt distribution of the final sample, leading to a biased \pt-differential yield. However, some of the variables used to train the model may be correlated with the invariant mass of the candidate, and the ML may learn to discriminate the signal from the background by exploiting this correlation with the \ds meson mass and transverse momentum, rather than the physical properties of the signal and background. To exclude this possibility, the correlation between the features used to train the model is studied. To quantitatively describe the correlation between the variables, the Pearson correlation coefficient $\rho$ is evaluated for each pair of variables. It is defined as the ratio between the covariance of two variables $x$ and $y$ and the product of their standard deviations, $\rho(x,y) = \mathrm{cov}(x,y)/(\sigma_{x}\sigma_{y})$. It expresses the strength and direction of a linear correlation between two variables, ranging from $\rho = 1$ (perfect positive linear correlation) to $\rho = -1$ (perfect negative linear relationship). $\rho = 0$ indicates the absence of linear correlation.

\begin{sloppypar}
The correlation matrix of the features used to train the model is shown in Fig.~\ref{fig:ml_training_vars} for the prompt \ds, non-prompt \ds and background classes, in the \mbox{$2<\pt<3$~\gevc}. The correlation with the invariant mass and the transverse momentum is also reported. The Pearson coefficient is encoded in the colour of the cell, with red indicating a positive correlation, blue a negative correlation, and grey no correlation. The correlation matrix shows that the variables used to train the model are not correlated with the invariant mass of the candidate, suggesting that a ML model should not modify the invariant-mass distribution of the selected candidates.
\end{sloppypar}

Variables carrying the same physical information, such as those related to the candidate decay length, pointing angle, and impact parameter, are strongly correlated among each other, as expected. Different degrees of correlation between the same variable pairs are observed for the different classes. The ML model can exploit these differences to discriminate between the three classes of candidates.

\begin{figure}[p]
    \centering
    \includegraphics[width=\textwidth]{Figures/Chapter 5/CorrelationMatrix.pdf}
    \caption{Correlation matrix of the features used to train the ML model for prompt \ds (top-left), non-prompt \ds (top-right), and background (bottom-left) candidates in the $2<\pt<3$~\gevc interval. The correlation with the invariant mass and the transverse momentum is also reported. The Pearson coefficient is encoded in the colour of the cell, with red indicating a positive correlation, blue a negative correlation, and grey no linear correlation.}
    \label{fig:ml_training_vars}
\end{figure}

\subsection{Boosted Decision Trees}
Once the training dataset has been composed and the features have been selected, the ML architecture has to be chosen. Several algorithms are available, each with its own strengths and weaknesses. The choice of the algorithm depends on the specific problem to solve, the size of the dataset, and the computational resources available. 

Boosted decision trees~\cite{friedman2001greedy,freund1997decision} (BDTs) are a family of machine learning algorithms employed in different fields, including high-energy physics. Their building blocks are decision trees, which are a versatile type of supervised learning algorithm that can be used for both classification and regression tasks. A decision tree is made of many \emph{nodes}, each containing conditions that split the data into two~\cite{breiman2017classification} or more~\cite{quinlan1986induction} children nodes. The first node of the tree, which receives all the data, is called \emph{root} node, while nodes that do not further split the data are called \emph{leaves}, and contain the output of the tree. The model is trained by considering the Gini index, which measures the impurity of the node:
\begin{equation*}
    G = 1 - \sum_{i=1}^{n} p_{i}^{2}\quad ,
\end{equation*}
where $p_{i}$ is the fraction of samples in the node that belong to the class $i$. The Gini index therefore provides an indication of the quality of the split, with $G=0$ indicating a perfect split. A commonly used algorithm to build \emph{binary} decision trees (i.e., each node contains binary-output conditions, and is split into two children nodes) is the \emph{Classification And Regression Tree} (CART) algorithm~\cite{breiman2017classification}, which recursively splits the dataset into subsets based on a single feature $k$ and a threshold $t_k$ that minimises the impurity of the subsets (weighted by their size). The cost function that the algorithm tries to minimise is given by
\begin{equation*}
    J(k,t_k) = \frac{m_{\mathrm{left}}}{m}G_{\mathrm{left}} + \frac{m_{\mathrm{right}}}{m}G_{\mathrm{right}}\quad ,
\end{equation*}
where $m_{\mathrm{left}}$ and $m_{\mathrm{right}}$ are the number of samples in the left and right nodes, respectively, summing up to the total number of samples $m$, and $G_{\mathrm{left}}$ and $G_{\mathrm{right}}$ are the Gini indices of the left and right nodes. The tree is grown until a stopping criterion is met, such as a maximum depth, a minimum number of samples in a node, or a minimum impurity decrease. These are all hyperparameters that can be tuned to optimise the model's performance.

Given their simplicity, decision trees are fairly easy to interpret, and are often called \emph{white-box} models (in contrast to BDTs and neural networks, where the decision-making process is less transparent, therefore called \emph{black-box} models). An additional strength of decision trees is that they require
very little data preparation, e.g., they do not require feature scaling or centering, making them a very powerful yet simple tool for data analysis. However, they are prone to overfitting, as they can grow to a large depth, capturing the noise in the training data. To mitigate this issue, their depth can be constrained, but this may lead to a model with limited discrimination power. To build a robust model with a good discrimination power, ensemble methods may be used. Several decision trees can be trained, and the final prediction is obtained by combining the outcome of all the trees. 

\subsubsection{XGBoost}
In this work, the Extrame Gradient Boosting~\cite{DBLP:xgboost} (XGBoost) Boosted Decision Trees (BDT) algorithm is used. It has achieved state-of-the-art results in a number of machine learning and data mining challenges (for example in Ref.~\cite{kaggle:higgs}). In addition, this algorithm, which is available as an open-source package, can be easily parallelised on CPUs and GPUs~\cite{mitchell2017accelerating}, thereby reducing the training and application time.

The term \emph{boosting} refers to any ensemble method combining several weak learners into a strong learner. The general idea of most boosting methods is to train many predictors sequentially, each trying to correct its predecessor~\cite{geron2022hands}. The function estimate $\widehat{f}(x)$ is parametrised with an additive functional form:
\begin{equation*}
    \widehat{f}(x) = \sum_{\mathrm{k}=1}^{\mathrm{M}} \widehat{f}_\mathrm{k}(x)\quad ,
\end{equation*}
where M is the number of iterations, $\widehat{f}_\mathrm{0}(x)$ is the initial prediction, and $\widehat{f}_\mathrm{i}(x)$ is the function increment at the $i$-th iteration, also called \emph{boost}. To reduce the loss function, a new weak learner, whose functional form is parametrised as $h(x,\theta)$, can be added to the ensemble:
\begin{equation*}
    \widehat{f}_\mathrm{t}(x) \leftarrow \widehat{f}_{\mathrm{t}-1}(x) + \rho_\mathrm{t} h(x,\theta_\mathrm{t})\quad .
\end{equation*}
$\rho_t$ is the step size, which is optimised for each iteration t, together with the parameters $\theta_\mathrm{t}$ of the weak learner:
\begin{equation*}
    (\rho_\mathrm{t}, \theta_\mathrm{t}) = \arg\min_{\rho,\theta} \sum_{{i}=1}^{\mathrm{N}} L\left(y_i, \widehat{f}_{\mathrm{t}-1}(x_i) + \rho h(x_i,\theta)\right)\quad ,
\end{equation*}
where $L$ is the loss function, and $y_i$ is the true label of the i-th example. Despite having a well-defined set of equations for minimising the loss function, the optimisation of the parameters is not trivial, as the loss function is non-convex and the search space is high-dimensional. Therefore, the optimisation is usually performed using a gradient-based algorithm~\cite{friedman2001greedy,natekin2013gradient}, where $h(x,\theta_\mathrm{t})$ is chosen as the most parallel function to the negative gradient of the loss function with respect to the previous prediction $g_\mathrm{t}(x)$:
\begin{equation*}
    g_\mathrm{t}(x) = E_{\mathbf{y}}\left[ \frac{\partial L(\mathbf{y},\widehat{f}_{\mathrm{t}-1}(x))}{\partial \widehat{f}_{\mathrm{t}-1}(x)} \Bigg{|} x \right] \quad ,
\end{equation*}
where $E_{\mathbf{y}}$ is the expectation over the true labels. The parameters are then optimised by minimising the difference between the negative gradient and the weak learner prediction:
\begin{equation*}
    (\rho_\mathrm{t}, \theta_\mathrm{t}) = \arg\min_{\rho,\theta} \sum_{\mathrm{i}=1}^{\mathrm{N}} \left[-g_\mathrm{t} - \rho h(x_\mathrm{i},\theta)\right]^{2}\quad .
\end{equation*}

Through the iterative addition of weak learners, the loss function is reduced and the model learns the complex patterns of data. The final prediction is obtained by summing the predictions of all the weak learners. In the XGBoost algorithm, the weak learners are decision trees. The output consists of a numerical score for each class, ranging from 0 to 1 and summing up to unity. Each score represents the confidence of the model in the prediction, which can be interpreted as the probability of the example belonging to that class.
\subsection{Tuning the model's hyperparameters}
The XGBoost algorithm has several hyperparameters~\cite{XGBoost_parameters} that can be tuned to optimise the model's performance. The most important hyperparameters are:
\begin{itemize}
    \item \code{eta} or \code{learning\_rate}, which is the step size shrinkage of the gradient descent algorithm. To reduce the risk of overfitting, this factor multiplies the weak-learner prediction ($\rho_\mathrm{t} h(x_\mathrm{i},\theta) \rightarrow \code{eta} \cdot \rho_\mathrm{t} h(x_\mathrm{i},\theta)$), and is usually set to a small value, such as 0.3;
    \item \code{max\_depth}, which is the maximum depth of the tree. A large depth can lead to overfitting, while a small depth can lead to a model with limited discrimination power;
    \item \code{n\_estimators}, which defines the number of trees to train. A large number of weak learners can lead to overfitting, while a small number can lead to a model with limited discrimination power. Usually, the number of weak learners is set to around 1000;
    \item \code{subsample}, which is the fraction of the training data to be used to train each tree at each iteration;
    \item \code{min\_child\_weight}, which is the minimum sum of instance weight needed in a child. It is related to the purity in a node, and it is used to stop the tree growth;
    \item \code{colsample\_bytree}, which is the fraction of features to be used to train each tree at each iteration;
    \item \code{tree\_method}, which defines the algorithm used to build the trees. The \code{hist} option uses an optimised histogram-based algorithm and is usually the fastest.
\end{itemize}

The hyperparameters are optimised using the Optuna framework~\cite{akiba2019optuna}, which proved to be a powerful tool thanks to its state-of-the-art algorithms for sampling the hyperparameter space and for efficiently pruning unpromising trials. The Tree-Structured Parzen Estimator~\cite{bergstra2011algorithms}, is used in this Thesis. It is a Bayesian optimisation algorithm able to explore the hyperparameter space efficiently. The aim of a Bayesian optimisation is to maximise (or minimise, depending on the task) an objective function $f(\mathbf{x})$ by iteratively sampling a bounded hyperparameter space, $\chi$. The algorithm builds a probabilistic model of the objective function, and uses it to decide which hyperparameters to sample next. The model is updated at each iteration, and the hyperparameters that are most likely to improve the model's performance are sampled. The Optuna algorithm is also able to prune unpromising trials, reducing the computational cost of the optimisation. The optimisation is performed using a 5-fold cross-validation, and the hyperparameters that maximise the macro-averaged one-vs-one ROC AUC metric (described in detail in Sec.~\ref{sec:ml_performance}) are chosen as the optimal configuration. The hyperparameters optimised for the XGBoost model are reported in Table~\ref{tab:ml_hyperparameters}. An additional hyperparameter, \code{lambda}, which is the $\mathrm{L_2}$ regularisation term, is also optimised. It helps to prevent overfitting by penalising overly complex models. The optimal hyperparameters are then used to train the model on the full training dataset.

\begin{table}[tb!]
    \centering
    \caption{Optimised hyperparameter configuration for the \pt bins considered in the model training.}
    \label{tab:ml_hyperparameters}
    \vspace*{0.3cm}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c|ccccccccc}
         \toprule
         Hyper-parameter & \multicolumn{9}{c}{$\pt$ interval (\gevc)} \\
         \midrule
          & 0--1.5 & 1.5--2 & 2--3 & 3--4 & 4--5 & 5--6 & 6--8 & 8--12 & 12--24 \\
        \midrule
         \code{max\textunderscore depth} & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3\\

         \code{learning\textunderscore rate} & 0.04 & 0.068 & 0.065 & 0.10 & 0.091 & 0.84 & 0.070 & 0.046 & 0.030\\
         
         \code{n\textunderscore estimators} & 473 & 339 & 1352 & 909 & 1256 & 1392 & 1142 & 1437 & 1188\\ 
         
         \code{min\textunderscore child\textunderscore weight} & 1 & 3 & 10 & 10 & 10 & 9 & 3 & 7 & 5\\
         
         \code{subsample} & 0.87 & 0.95 & 0.84 & 0.85 & 0.95 & 0.85 & 0.81 & 0.94 & 0.88\\
         
         \code{colsample\textunderscore bytree} & 0.91 & 0.98 & 0.90 & 0.98 & 0.96 & 0.95 & 0.88 & 0.96 & 0.89\\

         \code{lambda} & $8.0\times10^{-4}$ & $4.8\times10^{-4}$ & $9.1\times10^{-4}$ &  $1.4\times10^{-4}$  &  $3.0\times10^{-4}$ & $3.2\times10^{-4}$ & $1.9\times10^{-4}$ & $9.8\times10^{-4}$ & $6.7\times10^{-4}$\\
         $\code{tree\textunderscore method}$ & $\code{hist}$ & $\code{hist}$ & $\code{hist}$ & $\code{hist}$& $\code{hist}$& $\code{hist}$ & $\code{hist}$ & $\code{hist}$& $\code{hist}$\\
         \bottomrule
    \end{tabular}%
}
\end{table}

\subsection{Evaluation of the model's performance}\label{sec:ml_performance}
After training the model, its performance is evaluated on the test dataset. The model's performance can be assessed using a \emph{confusion matrix}, which summarises the number of examples for a given class (the true label) that are classified by the model as belonging to any of the available classes (the predicted label). A good model should provide a high number of correctly-classified examples (reported on the diagonal of the confusion matrix), and a low number of misclassified examples (off-diagonal elements of the confusion matrix). The confusion matrix also allows an understanding of which classes are more difficult to classify, and which classes are more likely to be confused with each other. An example of a confusion matrix is shown in Fig.~\ref{fig:ml_confusion_matrix} for the XGBoost model trained on the $2<\pt<3$~\gevc interval.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter 5/ConfusionMatrix.pdf}
    \caption{Confusion matrix for the BDT model trained in the \mbox{$2<\pt<3$~\gevc} interval. Candidates are classified as the class with the highest score.}
    \label{fig:ml_confusion_matrix}
\end{figure}

Despite providing a lot of information on the model's performance, more concise metrics of the model's performance are usually used, for a more direct comparison between different models. In addition, the confusion matrix provides a threshold-dependent measure of the model's performance, as the classification threshold can be varied to increase the number of correctly classified signal candidates at the expense of the number of correctly classified background candidates, and vice versa. 

In binary classification tasks, where only two classes are available (a positive and a negative class), several metrics can be defined from the elements of the confusion matrix. The $2\mkern-2mu\times\mkern-2mu2$ confusion matrix contains four entries: the true positives (TP), which are the number of correctly classified positive candidates, the false positives (FP), which are the number of negative candidates being mistakenly classified as positives, and the analogously defined true negatives (TN) and false negatives (FN). One of the most used tools for binary classifiers is the \emph{Receiver Operating Characteristic} (ROC) curve, which represents the true positive rate (TPR) against the false positive rate (FPR) for different threshold values. The TPR is the fraction of correctly classified positive candidates ($\mathrm{TPR = TP/(TP+FN)}$), while the FPR is the fraction of incorrectly classified negative candidates ($\mathrm{FPR = FP/(FP+TN)}$). If positive candidates are selected as those with a score greater than a certain threshold $t$, then when $t=0$ all candidates are classified as positive, and both the TPR and FPR will be equal to 1. On the other hand, if $t=1$, no candidate is classified as positive, and the TPR and FPR will both equal 0. The ROC \emph{Area Under the Curve} (AUC), is used to measure the model's ability to discriminate between positive and negative candidates, for any given threshold. The ROC AUC ranges from 0 to 1. A random classifier has a ROC AUC of 0.5, while a perfect classifier has a ROC AUC of 1. The ROC AUC is a threshold-independent measure of the model's performance, and is often used to compare different models. 

In a multiclass classification task, where more than two classes are available, a generalisation of the ROC curve and the ROC AUC metric is required. In this case, the \emph{One-vs-One} ROC curve can be defined as a plot of the TPR against the FPR for a single pair of classes. The One-vs-One ROC AUC can be averaged to the \emph{macro-averaged} One-vs-One ROC AUC, which is the average of the ROC AUC for each pair of classes and can provide a measurement of the model's ability to discriminate between all the classes.


The One-vs-One ROC curves for the model trained on the $2<\pt<3$~\gevc interval are shown in Fig.~\ref{fig:ml_roc_curve}. The ROC AUC is calculated for each class pair, and is reported in the legend. The metric is evaluated on both the training and test sets to test the model's generalisation power. The model's performance is excellent, with a macro-averaged One-vs-One ROC AUC value very close to 1. In addition, little overfitting is observed, as the ROC AUC values for the training and test sets are similar. The model is then used to select \ds and \dpl candidates from the full dataset. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter 5/ROC.pdf}
    \caption{ROC curves for the model trained on the $2<\pt<3$~\gevc interval. The One-vs-One ROC AUC metric is calculated for each class pair and reported in the legend.}
    \label{fig:ml_roc_curve}
\end{figure}

\begin{sloppypar}
In addition to the ROC AUC, the model's performance can be evaluated by studying the distribution of the probability of belonging to a given class assigned to labelleled candidates. The score distribution for the model trained on the \mbox{$2<\pt<3$~\gevc} interval is shown in Fig.~\ref{fig:ml_score} for the background, prompt \ds, and non-prompt \ds classes. For each class, the score distribution is shown for candidates belonging to the different classes and to both the training and test sets. The distribution of the background score provides interesting information on the model's performance. The score distribution for the background candidates peaks at high values, while the score distribution for the signal candidates (both prompt and non prompt \ds) peaks at low values. This highlights that the model has effectively learned to discriminate between signal and background candidates, with good separation power. Furthermore, the score distributions for the training and test sets are fairly similar, indicating that the model generalises well to unseen data. Since non-prompt \ds present a very displaced topology due to the large lifetime of beauty-hadrons, the separation between non-prompt \ds and background is noticeable in the non-prompt \ds score distribution. Finally, as the displacement of prompt candidates lies in between the non-prompt \ds and background, the separation between the three classes is less pronounced in the prompt \ds score distribution, where the prompt \ds distribution does not peak at values around one.
\end{sloppypar}

\begin{figure}[p]
    \centering
    \includegraphics[width=\textwidth]{Figures/Chapter 5/Distributions.pdf}
    \caption{Score distribution for the model trained on the $2<\pt<3$~\gevc interval. The score distribution related to the probability of belonging to the background, prompt \ds, and non-prompt \ds classes is shown. For each class, the score distribution is shown for candidates belonging to the different classes and to both the training (filled area) and test sets (markers).}
    \label{fig:ml_score}
\end{figure}

\subsection{Interpretation of the model's output: Feature importance}
The usage of ML algorithms usually provides a better performance in terms of signal-to-background separation, but it also introduces a level of complexity in the selection process. One of the most difficult aspects of using ML models is the interpretation of their output. To understand how the model makes its decisions, the feature importance can be studied. This allows the understanding of which features are more important for the model's decision-making process, and the optimisation of the feature selection. In addition, the feature importance can be used to check whether the model is learning on the correct features in terms of the physics of the problem.

One of the most used algorithms for feature importance studies is the SHapley Additive exPlanations~\cite{lundberg2017unified} (SHAP) algorithm. SHAP is a game-theoretic approach to explain the output of any machine learning model. It is based on the Shapley value~\cite{lipovetsky2001analysis} from cooperative game theory, which requires retraining the model on all feature subsets $\mathcal{S}\subseteq \mathcal{F}$, where $\mathcal{F}$ is the set of all features. An importance value is assigned to each feature, representing the effect on the model prediction of including that feature. To compute this effect, a model $\widehat{f}_{\mathcal{S}\cup\{i\}}$ is trained with that feature present, and another model $\widehat{f}_\mathcal{S}$ is trained with the feature withheld. Then, predictions from the two
models are compared on the current input \mbox{$\widehat{f}_{\mathcal{S}\cup\{i\}} (x) - \widehat{f}_\mathcal{S}$}. The Shapley values are then computed as the weighted average of all possible differences:
\begin{equation*}
    \phi_\mathrm{i} = \sum_{\mathcal{S}\subseteq \mathcal{F}\setminus\{i\}} \frac{|\mathcal{S}|!(|\mathcal{F}|-|\mathcal{S}|-1)!}{|\mathcal{F}|!} \left[\widehat{f}_{\mathcal{S}\cup\{i\}}(x) - \widehat{f}_\mathcal{S}(x)\right]\quad .
\end{equation*}
Since most models cannot handle arbitrary patterns of missing input values, $\widehat{f} (z_\mathcal{S})$ is approximated with $E[\widehat{f}(z) | z_\mathcal{S}]$, where $z_\mathcal{S}$ is the input missing the features in $\mathcal{S}$. SHAP values therefore explain how to get from the base value $E[\widehat{f}(z)]$ that would be predicted if no features were known to the output $\widehat{f}(x)$.

A beeswarm-style SHAP feature importance plot for the prompt \ds probability predicted by the model trained in the $2<\pt<3$~\gevc interval is shown in Fig.~\ref{fig:ml_feature_importance}. The most important features are the cosine of pointing angle, the decay length, the decay length in the XY plane, the cosine cubed of the K-$\pi$ angle in the KK rest frame, and the PID information on the prong 1. As discussed in Chapter~\ref{chap:reconstruction}, the first three features are related to the displaced topology of D mesons, and are therefore expected to be the most important variables in the model decisions. It is also expected that the prong 1 PID information resulted as the most important PID variable, as this is the opposite sign track, which is always a kaon in the considered decay channel. On the contrary, prongs 0 and 2 could be either kaons or pions, resulting in a lower importance of the PID information for these prongs. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{Figures/Chapter 5/shap.pdf}
    \caption{SHAP feature importance for the XGBoost model trained on the \mbox{$2<\pt<3$~\gevc} interval.}
    \label{fig:ml_feature_importance}
\end{figure}

\subsection{Optimisation of the model selection}
Once the model performance has been validated, a set of selection criteria has to be chosen to select the candidates. This is a crucial step of the analysis, as it will define the signal extraction efficiency and the background contamination. Since the model's output consists of a score related to the probability of belonging to each class, and the three probabilities sum up to unity, the selection criteria have a total of two degrees of freedom. A first selection is applied on the maximum probability to be a background candidate, and rejects most of the contamination from the combinatorial background. The second one is applied on the minimum probability of being a prompt \ds candidate, and suppresses the signal contribution arising from non-prompt \ds candidates. 

A first indication of the optimal selection criteria can be obtained by studying the model's output distributions. A good working point can be chosen as the point where a good separation between the three classes is achieved. For this analysis, however, the statistical significance of the signal is used to define the optimal selection criteria. For each \pt interval of the analysis, the signal $S$ and the background $B$ are evaluated by fitting the invariant mass distribution of candidates passing the different ML selections to be considered in the optimisation process. Only a subset of the full dataset, corresponding to $\sim 3\%$ of the available data sample is used in this process. The working point is chosen as the set of selections maximising the statistical significance $S/\sqrt{S+B}$. The efficiency of the selection is checked to ensure that it is kept at sufficiently high levels, to reduce possible biases in the final results due to possible imperfections in the MC description of the data. Usually, the optimisation of the statistical significance is avoided in the optimisation of the selection criteria, as it can lead to a bias in the final results. However, in this case, the optimisation is performed on a very small fraction of the data thanks to the large dataset available, and the bias is expected to be negligible.

A result of the working point optimisation is presented in Fig.~\ref{fig:ml_significance} for the \mbox{$4.0<\pt<4.5$~\gevc} interval. The statistical significance of the signal is shown as a function of the BDT output score threshold for the probability of being a prompt \ds and a background candidate. The chosen set of selection criteria is shown with a green cross.

The optimal selection criteria for each \pt interval considered in the analysis are reported in Table~\ref{tab:working_point}.


\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter 5/Significance_Scan.pdf}
    \caption{Statistical significance of the signal as a function of the selection criteria applied to the model output for the $4.0<\pt<4.5$~\gevc interval. The chosen set of selection criteria is shown with a green cross.}
    \label{fig:ml_significance}
\end{figure}

\begin{table}[b!]
    \centering
    \caption{Selection criteria applied to enhance the significance of the $\ds$ meson contribution in the $\pt$ bins of the analysis.}
    \label{tab:working_point}
    \vspace*{0.3cm}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c|cc}
    \toprule
         \pt interval (\gevc) & Probability to be background $<$ & Probability to be prompt \ds $>$  \\
         \midrule
         0.5$-$1.0 & 0.01 & 0.2 \\
         1.0$-$1.5 & 0.05 & 0.2 \\
         1.5$-$2.0 & 0.15 & 0.2 \\
         2.0$-$2.5 & 0.25 & 0.2 \\
         2.5$-$3.0 & 0.3 & 0.2 \\
         3.0$-$3.5 & 0.2 & 0.2 \\
         3.5$-$4.0 & 0.2 & 0.2 \\
         4.0$-$4.5 & 0.2 & 0.2 \\
         4.5$-$5.0 & 0.2 & 0.2 \\
         5.0$-$5.5 & 0.3 & 0.2 \\
         5.5$-$6.0 & 0.3 & 0.25 \\
         6$-$8   & 0.45 & 0.2 \\
         8$-$12  & 0.5 & 0.2 \\
         12$-$24 & 0.55 & 0.2 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\section{\texorpdfstring{\ds and \dpl raw-yield extraction}{Ds+ and D+ raw-yield extraction}}

After the working point for the BDT algorithm has been defined, the raw yields of \ds and \dpl mesons are extracted in each \pt interval. They are defined as the sum of particles and antiparticles and are measured in the $0.5<\pt<24$~\gevc interval. The raw yield is extracted by fitting the invariant mass distribution of the selected candidates. 

In several analyses performed by the ALICE Collaboration during the Run~2 data-taking period~\cite{ALICE:2021mgk,ALICE:2023sgl,ALICE:2021kfc}, the raw yield of \ds mesons was extracted by fitting the invariant mass distribution of selected candidates with a probability density function constructed as the sum of a function describing the shape of the combinatorial background (usually an exponential function or a low-order ($<3$) polynomial) and of two gaussian distributions to model the \ds and \dpl peaks. The raw yields for the two D-meson species are then obtained by integrating the signal function. 

Figure~\ref{fig:old_fit} shows the fit to the invariant mass distribution of the selected candidates using the approach described above. Due to the concavity-changing shape of the background, the function chosen to describe the background is a third-order polynomial. On top of the previously-unobserved peculiar shape of the background, the fitting function is not able to describe the data accurately between the two peaks, overestimating the data in this invariant mass region.
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/Chapter 5/InvMassFitDs1p5_2.pdf}
    \caption{Fit to the invariant mass distribution of selected candidates in the \mbox{$2<\pt<3$~\gevc} interval. The fit function is shown as a solid line, while the signal and background components are shown as dashed lines.}
    \label{fig:old_fit}
\end{figure}

These two features can be understood as due to the fact that the background does not solely arise from the combination of independent tracks, i.e. the combinatorial background. Other physics processes can contribute to the contamination of the data sample, giving rise to a \emph{correlated background}. One such process is the decay of \dpl mesons into the $\dpl\rightarrow\mathrm{\pi^+K^-\pi^+}$ decay channel, where one of the pions is misidentified as a kaon. Despite being suppressed by the applied ML selections, this contribution can give rise to a noticeable contribution due to the large BR of $9.38\times10^{-2}$~\cite{pdg}.

To validate this hypothesis, a simulation of \dpl decays into $\dpl\rightarrow\mathrm{\pi^+K^-\pi^+}$ was run using \textsc{Pythia}~8~\cite{Bierlich:2022pfr}. Ten billion \dpl mesons were produced with a uniformly distributed \pt spectrum in the $0<\pt<24$~\gevc interval. The \dpl mesons were then forced to decay into the $\dpl\rightarrow\mathrm{\pi^+K^-\pi^+}$ decay channel. For each decay, two invariant masses were evaluated, by assigning the kaon mass to one of the two pions, while the correct masses were assigned to the other two prongs. The invariant mass distributions obtained from the simulation are shown in Fig.~\ref{fig:DplusSimulations} for different \pt intervals. The distribution is characterised by a peak at $\sim 2$~\gevcc, well between the fit range used to extract the raw yield of \ds and \dpl mesons in Fig~\ref{fig:old_fit}. In addition, the invariant mass distribution evolves with the \dpl \pt, with a tail towards higher invariant masses for higher \pt values. This follows naturally from the kinematic properties of the decay.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Figures/Chapter 5/Dplus_Corr_Bkg_simulation.pdf}
    \caption{Invariant mass distribution simulated decays of \dpl mesons into \mbox{$\dpl\rightarrow\mathrm{\pi^+K^-\pi^+}$}, where one of the pions produced in the decay is misidentified as a kaon. Distributions for different \pt intervals are shown. }
    \label{fig:DplusSimulations}
\end{figure}

To account for this contribution, a template fit is included in the fit function. This method involves fitting a distribution using a predefined template whose shape is fixed, and the only adjustable parameter is the normalisation of the function. The shape of the template is taken from the same simulation used to train the BDT model, as described in Sec.~\ref{sec:ml_data_preparation}, where $\dpl\rightarrow\mathrm{\pi^+K^-\pi^+}$ mesons reconstructed as $\dpl\rightarrow\mathrm{K^+K^-\pi^+}$ are selected. The distribution is fixed to that obtained before applying any ML models, as it was studied that the applied ML selections do not affect the shape of the correlated background. This allows for reducing statistical fluctuations in the template shape. The final fit function is then constructed as the sum of a parabolic function to describe the combinatorial background, the template function described above for the correlated $\dpl\rightarrow\mathrm{\pi^+K^-\pi^+}$ background source, and two gaussian functions to describe the \ds and \dpl peaks. The signal parameters (mean, width, normalisation), as well as the parameters of the combinatorial background and the normalisation of the template fit are left free in the fit in the $\pt<8$~\gevc interval. Because of the observed momentum resolution worsening at higher \pt, and because of the lower statistics available in the high-\pt intervals, the width of the \dpl signal peak is fixed to that of the \ds divided by a factor 1.2, which is the observed ratio of the peak widths at low \pt, which presents a flat trend with \pt. In these high-\pt intervals, a first fit is performed by keeping both peak widths as free parameters. Then, the \dpl peak width is fixed to that of the \ds divided by 1.2, and the fit is repeated, keeping the remaining parameters free. Additionally, in order to correctly describe the background shape, the fit range is extended to $1.73<M(\mathrm{KK\pi})<2.15$~\gevcc from $\pt>6$~\gevc, while the narrower invariant mass window $1.75<M(\mathrm{KK\pi})<2.1$~\gevcc is fitted at lower \pt. The invariant-mass bin width has been fixed to 2~\mevcc. The fits to the invariant mass distributions of selected D-meson candidates are performed using the \code{flarefly} package~\cite{grosa_2023_7579657}, which provides a flexible pythonic interface for performing fits. 


The fit to the invariant mass distribution of candidates passing the ML selections is shown in Fig.~\ref{fig:new_fit} for the $2.0<\pt<2.5$~\gevc in the left panel, and for the {$5.0<\pt<5.5$~\gevc} interval in the right panel. The total fit function is shown as a solid blue line, the signal contributions are shown as filled green and azure areas for \dpl and \ds mesons, respectively, the combinatorial background is represented with a solid red line, while the correlated background is shown as a dashed violet line. The fit is able to describe the data accurately, as can be deduced from the distribution of the difference between the data and the background fit function, shown in the bottom panels for the two \pt bins. Figure~\ref{fig:new_fit} also shows that the contribution of the correlated $\dpl\rightarrow\mathrm{\pi^+K^-\pi^+}$ background evolves with \pt, with a larger contribution at lower \pt values. The raw yield of \ds and \dpl mesons is then extracted by integrating the signal function.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.48\textwidth]{Figures/Chapter 5/invmassfit_2_2p5.pdf}
    \includegraphics[width=0.48\textwidth]{Figures/Chapter 5/invmassfit_5_5p5.pdf}
    \caption{Fit to the invariant mass distribution of selected candidate in the \mbox{$2.0<\pt<2.5$~\gevc} (left) and \mbox{$5.0<\pt<5.5$~\gevc} (right) intervals. The total fit function is shown as a solid blue line, while background components are shown as solid red lines (correlated background) and dashed violet lines (dashed violet lines). The signal contributions are shown as filled green and azure areas for \dpl and \ds mesons, respectively. The bottom panels show the distribution of the difference between the data and the background fit function.}
    \label{fig:new_fit}
\end{figure}

The extraction of the raw yields is affected by several arbitrary choices, for example, the functional description of the background, the choice of the fit range, and the invariant-mass bin width. Changes in these choices can lead to variations in the extracted raw yields. To estimate the effect of such arbitrary choice in the final observable (the \ds/\dpl production-yield ratio), and estimate a systematic uncertainty associated with the raw yield extraction procedure, the fit is repeated several times by varying the fit range, the bin width, and the functional form of the background. In the low \pt region, the minimum mass is varied between 1.71 and 1.75~\gevcc, while the maximum mass is varied between 2.13 and 2.19~\gevcc. The functions considered to describe the background are a second-order polynomial and an exponential function. At higher \pt, where the signal peaks become broader, the minimum mass is varied between 1.71 and 1.75~\gevcc, while the maximum mass is varied between 2.13 and 2.19~\gevcc. The bin width is varied between 1 and 4~\mevcc across the studied \pt interval. At $\pt>8$~\gevcc, where the \dpl peak width is fixed to that of the \ds divided by 1.2, the peak width of the \dpl meson is changed by varying the dividing factor by $\pm10\%$. At lower \pt, all the fit parameters are left free, as for the central case. For each possible combination of these variations, the signal is extracted for the two D-meson species, and the ratio between the two is calculated. The systematic uncertainty is then defined as the sum in quadrature of the standard deviation of the distribution of the \ds/\dpl raw-yield ratio and of the difference between the central raw-yield ratio and the mean of the obtained distribution.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{Figures/Chapter 5/RawYieldSyst.pdf}
    \caption{Results from the multi-trial approach employed for estimating the systematic uncertainty related to the raw yield extraction in the \mbox{$1.0<\pt<1.5$~\gevc} interval.}
    \label{fig:raw_yield_syst}
\end{figure}

The result of this multi-trial approach for the evaluation of the systematic on the raw-yields extraction is shown in Fig.~\ref{fig:raw_yield_syst} for the $1.0<\pt<1.5$~\gevc interval. In the top-left panel, the raw yields extracted from the fit to the invariant mass distribution are reported for the \ds and \dpl mesons for the different trials. As a cross-check, the raw yields are also extracted by summing the counts of a distribution obtained by subtracting the background fit function from the invariant mass distribution of the candidates passing the ML selections. The bin contents are summed within 3 and 5 standard deviations from the peak mean. The extracted raw yields are stable within uncertainty across all the trials and the raw yield definition for the \dpl meson. For the \ds meson, the $5\sigma$ bin counting method presents higher raw yields when compared to the ones obtained by integrating the signal function or with a $3\sigma$ bin counting method. However, this is considered as related to fluctuation in the invariant mass distribution rather than a systematic shift of the raw yields due to a different definition of the observable. The raw yields extracted with the default configuration described above are shown as dashed lines. In the top-right panel, the $\chi^2/\mathrm{ndf}$ of the fit to the invariant mass distribution is shown for the different trials. As a quality check, trials with a $\chi^2/\mathrm{ndf}$ greater than 2 are discarded. These results illustrate that the fit function is able to describe the data accurately, independently of the configuration of the fit parameters. In the bottom-left panel, the peak widths for \ds and \dpl are reported, and present a stable behaviour across the different trials. The peak widths for the default configuration are also reported as dashed lines. The bottom-right panel shows the distribution of the \ds/\dpl raw-yield ratio for the different trials, obtained using the three methods previously introduced. The $5\sigma$ bin counting method presents a higher raw-yield ratio when compared to the other two methods, because of the higher \ds yields. The systematic uncertainty is defined as the sum in quadrature of the standard deviation of the distribution of \ds/\dpl raw-yield ratio obtained by integrating the signal function and the difference between the central value reported with a dashed red line and the mean of this distribution. This quantity ranges from 1\% to 10\% of the central \ds/\dpl raw-yield ratio, depending on the considered \pt interval. The systematic uncertainty is then assigned after smoothing the \pt dependence of the obtained values. The assigned systematic uncertainty is reported in Table~\ref{tab:raw_yield_syst}.




\begin{table}[htb]
    \centering
    \caption{Systematic uncertainty on the raw-yield extraction for the \ds and \dpl mesons.}
    \label{tab:raw_yield_syst}
    \vspace*{0.3cm}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c|cc}
    \toprule
         \pt (\gevc) & $\sqrt{\mathrm{RMS^2+\Delta^2}}$/(central \ds/\dpl) (\%) & Assigned systematic uncertainty (\%)\\
         \midrule
         0.5$-$1 & 4  & 3 \\
         1$-$1.5 & 1  & 3 \\
         1.5$-$2 & 2  & 3 \\
         2$-$2.5 & 3  & 3 \\
         2.5$-$3 & 3  & 3 \\
         3$-$3.5 & 3  & 3 \\
         3.5$-$4 & 3  & 3 \\
         4$-$4.5 & 6  & 5 \\
         4.5$-$5 & 7  & 5 \\
         5$-$5.5 & 3  & 5 \\
         5.5$-$6 & 5  & 5 \\
         6$-$8   & 8  & 8 \\
         8$-$12  & 9  & 9 \\
         12$-$24 & 10 & 10 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}
